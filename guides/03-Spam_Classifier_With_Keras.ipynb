{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360f5915",
   "metadata": {},
   "source": [
    "## Build a Spam Classifier with Keras\n",
    "With deep learning and AI, handling spam content has gotten easier and easier. Over time (and with the aid of direct user feedback) our spam classifier will rarely produce erroneous results.\n",
    "\n",
    "This is the first part of a multi-part series covering how to:\n",
    "\n",
    "- Build an AI Model (this one)\n",
    "- Integrate a NoSQL Database (inference result storing)\n",
    "- Deploy an AI Model into Production\n",
    "\n",
    "### Prerequisites\n",
    "- Prepare your dataset using this notebook .\n",
    "- Convert your dataset into trainable vectors in this notebook (Either way, this notebook will run this step for us).\n",
    "\n",
    "### Running this notebook:\n",
    "- Recommended: Use Colab as it offers free GPUs for training models. Launch this notebook here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8504c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b14b3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PROJECT_ROOT = True\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "if USE_PROJECT_ROOT:\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "DATASET_DIR = BASE_DIR / \"datasets\"\n",
    "EXPORT_DIR = DATASET_DIR / \"exports\"\n",
    "DATASET_CSV_PATH = EXPORT_DIR / 'spam-dataset.csv'\n",
    "\n",
    "GUIDES_DIR = BASE_DIR / \"guides\"\n",
    "TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'\n",
    "PART_TWO_GUIDE_PATH = GUIDES_DIR / \"02-Convert_Dataset_Into_Vectors.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32f4f2",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "Creating a dataset rarely happens next to where you run the training. The below cells are a method for us to extract the needed data to perform training against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abbdd3",
   "metadata": {},
   "source": [
    "```shell\n",
    "!mkdir -p \"$EXPORT_DIR\"\n",
    "!mkdir -p \"$GUIDES_DIR\"\n",
    "!curl \"https://github.com/KewJS/spam_classification/blob/master/data_local/exports/spam-dataset.csv\" -o \"$DATASET_CSV_PATH\"\n",
    "!curl \"https://github.com/KewJS/spam_classification/blob/master/nbs/02-Convert_Dataset_Into_Vectors.ipynb\" -o \"$PART_TWO_GUIDE_PATH\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0c04b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>sms-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>sms-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>sms-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>sms-spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>sms-spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text    source\n",
       "0   ham  Go until jurong point, crazy.. Available only ...  sms-spam\n",
       "1   ham                      Ok lar... Joking wif u oni...  sms-spam\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...  sms-spam\n",
       "3   ham  U dun say so early hor... U c already then say...  sms-spam\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...  sms-spam"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dea1b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR is F:\\KEW_JING_SHENG\\01-SELF_LEARNING\\02-Data_Science\\35-Spam_Classification\n",
      "Random Index 6582\n",
      "Found 9730 unique tokens.\n",
      "Done creating tokenized train & test data...\n"
     ]
    }
   ],
   "source": [
    "%run \"$PART_TWO_GUIDE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17cc6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "with open(TRAINING_DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2adca",
   "metadata": {},
   "source": [
    "> While the above code uses <code>pickle</code> to load in data, this data is actually exported via <code>pickle</code> when we execute the <code>%run</code> only a few steps ago. Since <code>pickle</code> can be unsafe to use from third-party downloaded data, we actually generate (again using <code>%run</code>) this pickle data and therefore is safe to use -- it's never downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e461940",
   "metadata": {},
   "source": [
    "## Transform Extracted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9186064",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data['X_test']\n",
    "X_train = data['X_train']\n",
    "y_test = data['y_test']\n",
    "y_train = data['y_train']\n",
    "labels_legend_inverted = data['labels_legend_inverted']\n",
    "legend = data['legend']\n",
    "max_sequence = data['max_sequence']\n",
    "max_words = data['max_words']\n",
    "tokenizer = data['tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36962b",
   "metadata": {},
   "source": [
    "## Create our LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4a132f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 300, 128)          35840     \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 300, 128)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,034\n",
      "Trainable params: 291,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS, embed_dim, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c87732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "154/154 [==============================] - 191s 1s/step - loss: 0.2728 - accuracy: 0.8864 - val_loss: 0.1335 - val_accuracy: 0.9586\n",
      "Epoch 2/5\n",
      "154/154 [==============================] - 155s 1s/step - loss: 0.1074 - accuracy: 0.9631 - val_loss: 0.0986 - val_accuracy: 0.9677\n",
      "Epoch 3/5\n",
      "154/154 [==============================] - 132s 861ms/step - loss: 0.1596 - accuracy: 0.9427 - val_loss: 0.1381 - val_accuracy: 0.9483\n",
      "Epoch 4/5\n",
      "154/154 [==============================] - 143s 931ms/step - loss: 0.0875 - accuracy: 0.9698 - val_loss: 0.0919 - val_accuracy: 0.9723\n",
      "Epoch 5/5\n",
      "154/154 [==============================] - 166s 1s/step - loss: 0.0703 - accuracy: 0.9761 - val_loss: 0.0944 - val_accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x166c50f0370>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, verbose=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "681cfcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x166e9cb0eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2a7e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EXPORT_PATH = EXPORT_DIR / 'spam-model.h5'\n",
    "model.save(str(MODEL_EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d62e26",
   "metadata": {},
   "source": [
    "## Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(text_str, max_words=280, max_sequence = 280, tokenizer=None):\n",
    "  if not tokenizer:\n",
    "    return None\n",
    "  sequences = tokenizer.texts_to_sequences([text_str])\n",
    "  x_input = pad_sequences(sequences, maxlen=max_sequence)\n",
    "  y_output = model.predict(x_input)\n",
    "  top_y_index = np.argmax(y_output)\n",
    "  preds = y_output[top_y_index]\n",
    "  labeled_preds = [{f\"{labels_legend_inverted[str(i)]}\": x} for i, x in enumerate(preds)]\n",
    "  return labeled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4239ef30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ham': 0.90141803}, {'spam': 0.09858193}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Buy me a new phone with discount\", max_words=max_words, max_sequence=max_sequence, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d4a24",
   "metadata": {},
   "source": [
    "## Exporting Tokenizer & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a07cc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"labels_legend_inverted\": labels_legend_inverted,\n",
    "    \"legend\": legend,\n",
    "    \"max_sequence\": max_sequence,\n",
    "    \"max_words\": max_words,\n",
    "}\n",
    "\n",
    "METADATA_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-metadata.json'\n",
    "METADATA_EXPORT_PATH.write_text(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d9204b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828992"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_as_json = tokenizer.to_json()\n",
    "\n",
    "TOKENIZER_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-tokenizer.json'\n",
    "TOKENIZER_EXPORT_PATH.write_text(tokenizer_as_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eb7f2",
   "metadata": {},
   "source": [
    "We can load <code>tokenizer_as_json</code> with <code>tensorflow.keras.preprocessing.text.tokenizer_from_json</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1044c",
   "metadata": {},
   "source": [
    "## Upload Model, Tokenizer, & Metadata to Object Storage\n",
    "\n",
    "\n",
    "Object Storage options include:\n",
    "- AWS S3\n",
    "- Linode Object Storage\n",
    "- DigitalOcean Spaces\n",
    "\n",
    "All three of these options can use <code>boto3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72b42274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Config\n",
    "ACCESS_KEY = \"\"\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "# No need to set in AWS\n",
    "ENDPOINT = None\n",
    "\n",
    "# Your s3-bucket region\n",
    "REGION = \"\"\n",
    "\n",
    "BUCKET_NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cd306",
   "metadata": {},
   "source": [
    "## Perform Upload with Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b524da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = ACCESS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea66e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KEY_NAME = f\"exports/spam-sms/{MODEL_EXPORT_PATH.name}\"\n",
    "TOKENIZER_KEY_NAME = f\"exports/spam-sms/{TOKENIZER_EXPORT_PATH.name}\"\n",
    "METADATA_KEY_NAME = f\"exports/spam-sms/{METADATA_EXPORT_PATH.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28610b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "client = session.client(\"s3\", region_name=REGION, endpoint_url=ENDPOINT)\n",
    "client.upload_file(str(MODEL_EXPORT_PATH), BUCKET_NAME, MODEL_KEY_NAME)\n",
    "client.upload_file(str(TOKENIZER_EXPORT_PATH), BUCKET_NAME, TOKENIZER_KEY_NAME)\n",
    "client.upload_file(str(METADATA_EXPORT_PATH), BUCKET_NAME, METADATA_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bfa7c",
   "metadata": {},
   "source": [
    "client.download_file(str(MODEL_EXPORT_PATH), BUCKET_NAME, MODEL_KEY_NAME)\n",
    "client.download_file(str(TOKENIZER_EXPORT_PATH), BUCKET_NAME, TOKENIZER_KEY_NAME)\n",
    "client.download_file(str(METADATA_EXPORT_PATH), BUCKET_NAME, METADATA_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20995af5",
   "metadata": {},
   "source": [
    "# Implement an AI Model Download Pipeline\n",
    "\n",
    "In this part, we will turn to <code>client.download_file()</code> portion into a pipeline so we can make it reusable in future projects. Further, if we ever need to bundle these models into a Docket image, we can use this pipeline created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51501cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam_classification",
   "language": "python",
   "name": "spam_classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
